{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre = pd.read_csv(\"../data/wine.csv\", header = None)\n",
    "df_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8000e+00, 1.4000e-01, 1.8000e-01, 1.4000e+00, 4.7000e-02,\n",
       "       3.0000e+01, 9.0000e+01, 9.9164e-01, 3.2700e+00, 5.4000e-01,\n",
       "       1.1200e+01, 0.0000e+00])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_pre.sample (frac=0.20)\n",
    "\n",
    "# 와인등급은 11번째 칼럼이므로 X에 11번째를 제외한 나머지를 넣는 과정이 까다롭다.\n",
    "dataset_1 = df.values\n",
    "dataset_2 = df.values\n",
    "X = np.delete(dataset_1, 11, axis=1)\n",
    "Y = dataset_2[:,11]\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_encoded = np_utils.to_categorical(Y)\n",
    "Y_encoded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim = 12, activation = \"relu\"))\n",
    "# model.add(Dense(24, activation = \"relu\"))\n",
    "# model.add(Dense(12, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "             optimizer = \"adam\",\n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DIR = \"./model1/\"\n",
    "# if not os.path.exists(MODEL_DIR):\n",
    "  #  os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# modelpath = \"./model1/{epoch:02d}-{val_loss:4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping (monitor = \"val_loss\", verbose=0, patience = 100)\n",
    "\n",
    "# checkpointer = ModelCheckpoint(filepath = modelpath, monitor = \"val_loss\", verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 779 samples, validate on 520 samples\n",
      "Epoch 1/2000\n",
      "779/779 [==============================] - 0s 505us/step - loss: 8.1676 - acc: 0.2131 - val_loss: 2.3714 - val_acc: 0.2712\n",
      "Epoch 2/2000\n",
      "779/779 [==============================] - 0s 82us/step - loss: 1.8685 - acc: 0.3543 - val_loss: 1.5325 - val_acc: 0.3885\n",
      "Epoch 3/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.4415 - acc: 0.4249 - val_loss: 1.4758 - val_acc: 0.3827\n",
      "Epoch 4/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.4396 - acc: 0.4275 - val_loss: 1.5282 - val_acc: 0.4635\n",
      "Epoch 5/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.4126 - acc: 0.4300 - val_loss: 1.3551 - val_acc: 0.4481\n",
      "Epoch 6/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.3724 - acc: 0.4275 - val_loss: 1.4882 - val_acc: 0.2788\n",
      "Epoch 7/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.3483 - acc: 0.4377 - val_loss: 1.3182 - val_acc: 0.4692\n",
      "Epoch 8/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.3421 - acc: 0.4390 - val_loss: 1.3757 - val_acc: 0.4577\n",
      "Epoch 9/2000\n",
      "779/779 [==============================] - 0s 94us/step - loss: 1.4656 - acc: 0.4223 - val_loss: 1.3567 - val_acc: 0.4442\n",
      "Epoch 10/2000\n",
      "779/779 [==============================] - 0s 85us/step - loss: 1.3556 - acc: 0.4416 - val_loss: 1.3894 - val_acc: 0.4442\n",
      "Epoch 11/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.4021 - acc: 0.4339 - val_loss: 1.4436 - val_acc: 0.4385\n",
      "Epoch 12/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.3687 - acc: 0.4570 - val_loss: 1.3852 - val_acc: 0.4308\n",
      "Epoch 13/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.3906 - acc: 0.4198 - val_loss: 1.3887 - val_acc: 0.4769\n",
      "Epoch 14/2000\n",
      "779/779 [==============================] - 0s 82us/step - loss: 1.4640 - acc: 0.4198 - val_loss: 1.6358 - val_acc: 0.4404\n",
      "Epoch 15/2000\n",
      "779/779 [==============================] - 0s 85us/step - loss: 1.3765 - acc: 0.4159 - val_loss: 1.4227 - val_acc: 0.4385\n",
      "Epoch 16/2000\n",
      "779/779 [==============================] - 0s 83us/step - loss: 1.3020 - acc: 0.4480 - val_loss: 1.3222 - val_acc: 0.4577\n",
      "Epoch 17/2000\n",
      "779/779 [==============================] - 0s 83us/step - loss: 1.2982 - acc: 0.4480 - val_loss: 1.2862 - val_acc: 0.4750\n",
      "Epoch 18/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.3679 - acc: 0.4352 - val_loss: 1.4040 - val_acc: 0.4288\n",
      "Epoch 19/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.3243 - acc: 0.4365 - val_loss: 1.2730 - val_acc: 0.4712\n",
      "Epoch 20/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.3241 - acc: 0.4531 - val_loss: 1.2902 - val_acc: 0.5135\n",
      "Epoch 21/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.3691 - acc: 0.4596 - val_loss: 1.2590 - val_acc: 0.4865\n",
      "Epoch 22/2000\n",
      "779/779 [==============================] - ETA: 0s - loss: 0.9117 - acc: 0.500 - 0s 76us/step - loss: 1.2970 - acc: 0.4454 - val_loss: 1.3565 - val_acc: 0.4673\n",
      "Epoch 23/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.3393 - acc: 0.4236 - val_loss: 1.3515 - val_acc: 0.4250\n",
      "Epoch 24/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2834 - acc: 0.4416 - val_loss: 1.3261 - val_acc: 0.4654\n",
      "Epoch 25/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2867 - acc: 0.4685 - val_loss: 1.3240 - val_acc: 0.4596\n",
      "Epoch 26/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2950 - acc: 0.4454 - val_loss: 1.2661 - val_acc: 0.4712\n",
      "Epoch 27/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.3072 - acc: 0.4377 - val_loss: 1.3343 - val_acc: 0.4558\n",
      "Epoch 28/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.5127 - acc: 0.4198 - val_loss: 1.4570 - val_acc: 0.4173\n",
      "Epoch 29/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.3452 - acc: 0.4724 - val_loss: 1.2681 - val_acc: 0.5173\n",
      "Epoch 30/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.3008 - acc: 0.4596 - val_loss: 1.3641 - val_acc: 0.4423\n",
      "Epoch 31/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.3054 - acc: 0.4570 - val_loss: 1.2776 - val_acc: 0.4942\n",
      "Epoch 32/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.3144 - acc: 0.4313 - val_loss: 1.3554 - val_acc: 0.5038\n",
      "Epoch 33/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.3269 - acc: 0.4429 - val_loss: 1.2586 - val_acc: 0.5154\n",
      "Epoch 34/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2433 - acc: 0.4724 - val_loss: 1.4010 - val_acc: 0.4192\n",
      "Epoch 35/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2867 - acc: 0.4544 - val_loss: 1.2742 - val_acc: 0.4750\n",
      "Epoch 36/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2419 - acc: 0.4840 - val_loss: 1.3271 - val_acc: 0.4712\n",
      "Epoch 37/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.2642 - acc: 0.4608 - val_loss: 1.2452 - val_acc: 0.5212\n",
      "Epoch 38/2000\n",
      "779/779 [==============================] - 0s 81us/step - loss: 1.2442 - acc: 0.4737 - val_loss: 1.2686 - val_acc: 0.5077\n",
      "Epoch 39/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.2876 - acc: 0.4673 - val_loss: 1.4154 - val_acc: 0.4346\n",
      "Epoch 40/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.2903 - acc: 0.4685 - val_loss: 1.3401 - val_acc: 0.4519\n",
      "Epoch 41/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.2729 - acc: 0.4583 - val_loss: 1.2665 - val_acc: 0.4962\n",
      "Epoch 42/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2440 - acc: 0.4788 - val_loss: 1.2387 - val_acc: 0.4865\n",
      "Epoch 43/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.2610 - acc: 0.4698 - val_loss: 1.2516 - val_acc: 0.4885\n",
      "Epoch 44/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2799 - acc: 0.4608 - val_loss: 1.2875 - val_acc: 0.4442\n",
      "Epoch 45/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.2582 - acc: 0.4750 - val_loss: 1.3479 - val_acc: 0.4942\n",
      "Epoch 46/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2992 - acc: 0.4660 - val_loss: 1.5356 - val_acc: 0.4635\n",
      "Epoch 47/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.3160 - acc: 0.4981 - val_loss: 1.3950 - val_acc: 0.4500\n",
      "Epoch 48/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2458 - acc: 0.4608 - val_loss: 1.2577 - val_acc: 0.4865\n",
      "Epoch 49/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2453 - acc: 0.4775 - val_loss: 1.2697 - val_acc: 0.4962\n",
      "Epoch 50/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2616 - acc: 0.4596 - val_loss: 1.2822 - val_acc: 0.4865\n",
      "Epoch 51/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.2638 - acc: 0.4724 - val_loss: 1.2944 - val_acc: 0.4577\n",
      "Epoch 52/2000\n",
      "779/779 [==============================] - 0s 86us/step - loss: 1.2546 - acc: 0.4685 - val_loss: 1.2559 - val_acc: 0.4846\n",
      "Epoch 53/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2688 - acc: 0.4596 - val_loss: 1.2547 - val_acc: 0.4942\n",
      "Epoch 54/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2394 - acc: 0.4596 - val_loss: 1.2315 - val_acc: 0.5135\n",
      "Epoch 55/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2430 - acc: 0.4673 - val_loss: 1.3635 - val_acc: 0.4577\n",
      "Epoch 56/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2696 - acc: 0.4531 - val_loss: 1.2316 - val_acc: 0.5135\n",
      "Epoch 57/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2560 - acc: 0.4583 - val_loss: 1.3052 - val_acc: 0.4769\n",
      "Epoch 58/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.2428 - acc: 0.4840 - val_loss: 1.2739 - val_acc: 0.4962\n",
      "Epoch 59/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2405 - acc: 0.4634 - val_loss: 1.2381 - val_acc: 0.5154\n",
      "Epoch 60/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 0s 69us/step - loss: 1.2303 - acc: 0.4801 - val_loss: 1.2216 - val_acc: 0.5019\n",
      "Epoch 61/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2220 - acc: 0.4955 - val_loss: 1.2287 - val_acc: 0.5019\n",
      "Epoch 62/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.3478 - acc: 0.4596 - val_loss: 1.3523 - val_acc: 0.4538\n",
      "Epoch 63/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2349 - acc: 0.4634 - val_loss: 1.3859 - val_acc: 0.4385\n",
      "Epoch 64/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2501 - acc: 0.4852 - val_loss: 1.2717 - val_acc: 0.4885\n",
      "Epoch 65/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2667 - acc: 0.4596 - val_loss: 1.2980 - val_acc: 0.4827\n",
      "Epoch 66/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2505 - acc: 0.4673 - val_loss: 1.2429 - val_acc: 0.5135\n",
      "Epoch 67/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2714 - acc: 0.4621 - val_loss: 1.2507 - val_acc: 0.5038\n",
      "Epoch 68/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.2366 - acc: 0.5109 - val_loss: 1.6510 - val_acc: 0.4365\n",
      "Epoch 69/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2391 - acc: 0.4750 - val_loss: 1.3054 - val_acc: 0.4712\n",
      "Epoch 70/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2531 - acc: 0.4673 - val_loss: 1.3481 - val_acc: 0.4288\n",
      "Epoch 71/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.2254 - acc: 0.4724 - val_loss: 1.2937 - val_acc: 0.4673\n",
      "Epoch 72/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.2086 - acc: 0.4852 - val_loss: 1.3482 - val_acc: 0.4673\n",
      "Epoch 73/2000\n",
      "779/779 [==============================] - 0s 83us/step - loss: 1.2355 - acc: 0.4557 - val_loss: 1.3301 - val_acc: 0.4654\n",
      "Epoch 74/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2407 - acc: 0.4698 - val_loss: 1.3538 - val_acc: 0.4558\n",
      "Epoch 75/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.1914 - acc: 0.4865 - val_loss: 1.2442 - val_acc: 0.5058\n",
      "Epoch 76/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2018 - acc: 0.4775 - val_loss: 1.4184 - val_acc: 0.4577\n",
      "Epoch 77/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.2123 - acc: 0.4660 - val_loss: 1.2672 - val_acc: 0.4981\n",
      "Epoch 78/2000\n",
      "779/779 [==============================] - 0s 81us/step - loss: 1.2571 - acc: 0.4865 - val_loss: 1.3085 - val_acc: 0.4712\n",
      "Epoch 79/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2293 - acc: 0.4621 - val_loss: 1.2708 - val_acc: 0.4827\n",
      "Epoch 80/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.2016 - acc: 0.5019 - val_loss: 1.3633 - val_acc: 0.4442\n",
      "Epoch 81/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1884 - acc: 0.4891 - val_loss: 1.2608 - val_acc: 0.4904\n",
      "Epoch 82/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.2184 - acc: 0.4801 - val_loss: 1.2317 - val_acc: 0.5269\n",
      "Epoch 83/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1651 - acc: 0.4852 - val_loss: 1.3615 - val_acc: 0.4904\n",
      "Epoch 84/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1988 - acc: 0.4814 - val_loss: 1.3004 - val_acc: 0.4904\n",
      "Epoch 85/2000\n",
      "779/779 [==============================] - 0s 78us/step - loss: 1.2224 - acc: 0.4647 - val_loss: 1.3242 - val_acc: 0.4558\n",
      "Epoch 86/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2151 - acc: 0.4775 - val_loss: 1.3376 - val_acc: 0.4327\n",
      "Epoch 87/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2459 - acc: 0.4775 - val_loss: 1.2349 - val_acc: 0.5250\n",
      "Epoch 88/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2661 - acc: 0.4968 - val_loss: 1.2993 - val_acc: 0.5000\n",
      "Epoch 89/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2146 - acc: 0.4660 - val_loss: 1.2623 - val_acc: 0.5115\n",
      "Epoch 90/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1735 - acc: 0.4852 - val_loss: 1.2474 - val_acc: 0.5288\n",
      "Epoch 91/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2035 - acc: 0.4840 - val_loss: 1.2474 - val_acc: 0.4808\n",
      "Epoch 92/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.1996 - acc: 0.4801 - val_loss: 1.3248 - val_acc: 0.4788\n",
      "Epoch 93/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1854 - acc: 0.4878 - val_loss: 1.2539 - val_acc: 0.5058\n",
      "Epoch 94/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.2232 - acc: 0.4955 - val_loss: 1.2694 - val_acc: 0.5154\n",
      "Epoch 95/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2400 - acc: 0.4827 - val_loss: 1.5005 - val_acc: 0.3404\n",
      "Epoch 96/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2823 - acc: 0.4788 - val_loss: 1.3105 - val_acc: 0.4712\n",
      "Epoch 97/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.2233 - acc: 0.4942 - val_loss: 1.2879 - val_acc: 0.5000\n",
      "Epoch 98/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1896 - acc: 0.4878 - val_loss: 1.2620 - val_acc: 0.4750\n",
      "Epoch 99/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.1832 - acc: 0.4955 - val_loss: 1.5404 - val_acc: 0.4519\n",
      "Epoch 100/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.2389 - acc: 0.4775 - val_loss: 1.2662 - val_acc: 0.5173\n",
      "Epoch 101/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1975 - acc: 0.5160 - val_loss: 1.2797 - val_acc: 0.5096\n",
      "Epoch 102/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2155 - acc: 0.4750 - val_loss: 1.3218 - val_acc: 0.4154\n",
      "Epoch 103/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.1740 - acc: 0.5135 - val_loss: 1.2569 - val_acc: 0.5115\n",
      "Epoch 104/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2010 - acc: 0.4673 - val_loss: 1.3411 - val_acc: 0.4846\n",
      "Epoch 105/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1748 - acc: 0.4968 - val_loss: 1.2737 - val_acc: 0.4769\n",
      "Epoch 106/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1710 - acc: 0.4852 - val_loss: 1.2672 - val_acc: 0.4885\n",
      "Epoch 107/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.2153 - acc: 0.4801 - val_loss: 1.2834 - val_acc: 0.4942\n",
      "Epoch 108/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2424 - acc: 0.4814 - val_loss: 1.2533 - val_acc: 0.5154\n",
      "Epoch 109/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.2412 - acc: 0.4763 - val_loss: 1.5359 - val_acc: 0.4308\n",
      "Epoch 110/2000\n",
      "779/779 [==============================] - 0s 73us/step - loss: 1.2308 - acc: 0.4647 - val_loss: 1.5253 - val_acc: 0.4442\n",
      "Epoch 111/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2713 - acc: 0.4608 - val_loss: 1.2949 - val_acc: 0.4942\n",
      "Epoch 112/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.1703 - acc: 0.4981 - val_loss: 1.3116 - val_acc: 0.4750\n",
      "Epoch 113/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.2158 - acc: 0.4737 - val_loss: 1.2690 - val_acc: 0.5038\n",
      "Epoch 114/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1805 - acc: 0.4917 - val_loss: 1.3833 - val_acc: 0.4538\n",
      "Epoch 115/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1745 - acc: 0.4891 - val_loss: 1.2548 - val_acc: 0.5038\n",
      "Epoch 116/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1833 - acc: 0.4955 - val_loss: 1.2999 - val_acc: 0.4962\n",
      "Epoch 117/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2100 - acc: 0.4788 - val_loss: 1.3112 - val_acc: 0.4731\n",
      "Epoch 118/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1641 - acc: 0.4929 - val_loss: 1.3377 - val_acc: 0.4096\n",
      "Epoch 119/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1625 - acc: 0.5122 - val_loss: 1.2617 - val_acc: 0.5019\n",
      "Epoch 120/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 0s 72us/step - loss: 1.1661 - acc: 0.5122 - val_loss: 1.3110 - val_acc: 0.4962\n",
      "Epoch 121/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1717 - acc: 0.4981 - val_loss: 1.3222 - val_acc: 0.4769\n",
      "Epoch 122/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1493 - acc: 0.5045 - val_loss: 1.2461 - val_acc: 0.5019\n",
      "Epoch 123/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.2726 - acc: 0.4570 - val_loss: 1.2674 - val_acc: 0.4904\n",
      "Epoch 124/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1751 - acc: 0.5045 - val_loss: 1.2508 - val_acc: 0.4865\n",
      "Epoch 125/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1871 - acc: 0.4763 - val_loss: 1.3780 - val_acc: 0.4788\n",
      "Epoch 126/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1843 - acc: 0.5006 - val_loss: 1.2422 - val_acc: 0.4769\n",
      "Epoch 127/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1580 - acc: 0.4878 - val_loss: 1.2260 - val_acc: 0.5077\n",
      "Epoch 128/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1644 - acc: 0.4981 - val_loss: 1.2758 - val_acc: 0.5115\n",
      "Epoch 129/2000\n",
      "779/779 [==============================] - 0s 86us/step - loss: 1.1663 - acc: 0.5058 - val_loss: 1.2690 - val_acc: 0.4692\n",
      "Epoch 130/2000\n",
      "779/779 [==============================] - 0s 81us/step - loss: 1.1645 - acc: 0.4801 - val_loss: 1.2803 - val_acc: 0.4846\n",
      "Epoch 131/2000\n",
      "779/779 [==============================] - 0s 81us/step - loss: 1.1599 - acc: 0.5019 - val_loss: 1.3503 - val_acc: 0.4519\n",
      "Epoch 132/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.1900 - acc: 0.4827 - val_loss: 1.3392 - val_acc: 0.4365\n",
      "Epoch 133/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1929 - acc: 0.4750 - val_loss: 1.2554 - val_acc: 0.5135\n",
      "Epoch 134/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1418 - acc: 0.4763 - val_loss: 1.2262 - val_acc: 0.5288\n",
      "Epoch 135/2000\n",
      "779/779 [==============================] - 0s 83us/step - loss: 1.1850 - acc: 0.4737 - val_loss: 1.3262 - val_acc: 0.4731\n",
      "Epoch 136/2000\n",
      "779/779 [==============================] - 0s 83us/step - loss: 1.1587 - acc: 0.4904 - val_loss: 1.2425 - val_acc: 0.5019\n",
      "Epoch 137/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.1600 - acc: 0.5096 - val_loss: 1.3418 - val_acc: 0.4596\n",
      "Epoch 138/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1657 - acc: 0.4852 - val_loss: 1.2906 - val_acc: 0.4885\n",
      "Epoch 139/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.1413 - acc: 0.5096 - val_loss: 1.3024 - val_acc: 0.5269\n",
      "Epoch 140/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.1662 - acc: 0.4878 - val_loss: 1.2549 - val_acc: 0.4846\n",
      "Epoch 141/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1803 - acc: 0.4942 - val_loss: 1.2675 - val_acc: 0.5038\n",
      "Epoch 142/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1365 - acc: 0.5186 - val_loss: 1.2544 - val_acc: 0.5000\n",
      "Epoch 143/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1530 - acc: 0.4994 - val_loss: 1.3009 - val_acc: 0.5000\n",
      "Epoch 144/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1948 - acc: 0.5083 - val_loss: 1.3855 - val_acc: 0.4577\n",
      "Epoch 145/2000\n",
      "779/779 [==============================] - 0s 71us/step - loss: 1.1799 - acc: 0.4852 - val_loss: 1.2959 - val_acc: 0.4865\n",
      "Epoch 146/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1729 - acc: 0.4685 - val_loss: 1.3235 - val_acc: 0.4904\n",
      "Epoch 147/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1693 - acc: 0.4917 - val_loss: 1.3103 - val_acc: 0.4692\n",
      "Epoch 148/2000\n",
      "779/779 [==============================] - 0s 76us/step - loss: 1.1613 - acc: 0.5212 - val_loss: 1.3761 - val_acc: 0.4519\n",
      "Epoch 149/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1832 - acc: 0.4775 - val_loss: 1.2375 - val_acc: 0.5231\n",
      "Epoch 150/2000\n",
      "779/779 [==============================] - 0s 72us/step - loss: 1.1441 - acc: 0.4968 - val_loss: 1.2413 - val_acc: 0.5192\n",
      "Epoch 151/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1490 - acc: 0.4942 - val_loss: 1.3537 - val_acc: 0.4519\n",
      "Epoch 152/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1507 - acc: 0.5096 - val_loss: 1.2683 - val_acc: 0.4865\n",
      "Epoch 153/2000\n",
      "779/779 [==============================] - 0s 69us/step - loss: 1.1345 - acc: 0.4955 - val_loss: 1.3799 - val_acc: 0.4192\n",
      "Epoch 154/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.1483 - acc: 0.5160 - val_loss: 1.3712 - val_acc: 0.4538\n",
      "Epoch 155/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1344 - acc: 0.5186 - val_loss: 1.2842 - val_acc: 0.4904\n",
      "Epoch 156/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1325 - acc: 0.4955 - val_loss: 1.2373 - val_acc: 0.5231\n",
      "Epoch 157/2000\n",
      "779/779 [==============================] - 0s 74us/step - loss: 1.1569 - acc: 0.5032 - val_loss: 1.3454 - val_acc: 0.4962\n",
      "Epoch 158/2000\n",
      "779/779 [==============================] - 0s 77us/step - loss: 1.1709 - acc: 0.4763 - val_loss: 1.2502 - val_acc: 0.5288\n",
      "Epoch 159/2000\n",
      "779/779 [==============================] - 0s 68us/step - loss: 1.1209 - acc: 0.4891 - val_loss: 1.3275 - val_acc: 0.4692\n",
      "Epoch 160/2000\n",
      "779/779 [==============================] - 0s 80us/step - loss: 1.1390 - acc: 0.5032 - val_loss: 1.3025 - val_acc: 0.5058\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y_encoded, validation_split = 0.4, epochs=2000, batch_size=10, \n",
    "                    callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1949/1949 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6033863518224611"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,Y_encoded)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 545 samples, validate on 364 samples\n",
      "Epoch 1/300\n",
      "545/545 [==============================] - 0s 317us/step - loss: 0.8867 - acc: 0.6422 - val_loss: 1.3597 - val_acc: 0.4780\n",
      "Epoch 2/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8779 - acc: 0.6404 - val_loss: 1.3809 - val_acc: 0.4863\n",
      "Epoch 3/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8916 - acc: 0.6239 - val_loss: 1.3548 - val_acc: 0.4918\n",
      "Epoch 4/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8900 - acc: 0.6257 - val_loss: 1.3768 - val_acc: 0.4780\n",
      "Epoch 5/300\n",
      "545/545 [==============================] - 0s 86us/step - loss: 0.8875 - acc: 0.6385 - val_loss: 1.4721 - val_acc: 0.4890\n",
      "Epoch 6/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8926 - acc: 0.6367 - val_loss: 1.3205 - val_acc: 0.4890\n",
      "Epoch 7/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8879 - acc: 0.6385 - val_loss: 1.3322 - val_acc: 0.5082\n",
      "Epoch 8/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8783 - acc: 0.6275 - val_loss: 1.3349 - val_acc: 0.4780\n",
      "Epoch 9/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8833 - acc: 0.6330 - val_loss: 1.4044 - val_acc: 0.4725\n",
      "Epoch 10/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8978 - acc: 0.6349 - val_loss: 1.3263 - val_acc: 0.4753\n",
      "Epoch 11/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8804 - acc: 0.6495 - val_loss: 1.4634 - val_acc: 0.4753\n",
      "Epoch 12/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8868 - acc: 0.6459 - val_loss: 1.3760 - val_acc: 0.4670\n",
      "Epoch 13/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8814 - acc: 0.6239 - val_loss: 1.4194 - val_acc: 0.4753\n",
      "Epoch 14/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8837 - acc: 0.6422 - val_loss: 1.3643 - val_acc: 0.4780\n",
      "Epoch 15/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8855 - acc: 0.6404 - val_loss: 1.4071 - val_acc: 0.4698\n",
      "Epoch 16/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8900 - acc: 0.6422 - val_loss: 1.3405 - val_acc: 0.4780\n",
      "Epoch 17/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8859 - acc: 0.6367 - val_loss: 1.3220 - val_acc: 0.5000\n",
      "Epoch 18/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.9019 - acc: 0.6275 - val_loss: 1.3797 - val_acc: 0.4808\n",
      "Epoch 19/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8961 - acc: 0.6404 - val_loss: 1.4701 - val_acc: 0.4670\n",
      "Epoch 20/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.9290 - acc: 0.6183 - val_loss: 1.4219 - val_acc: 0.4780\n",
      "Epoch 21/300\n",
      "545/545 [==============================] - 0s 106us/step - loss: 0.9030 - acc: 0.6294 - val_loss: 1.3335 - val_acc: 0.5192\n",
      "Epoch 22/300\n",
      "545/545 [==============================] - 0s 92us/step - loss: 0.8861 - acc: 0.6587 - val_loss: 1.3330 - val_acc: 0.4890\n",
      "Epoch 23/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.9142 - acc: 0.6183 - val_loss: 1.3462 - val_acc: 0.4670\n",
      "Epoch 24/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8824 - acc: 0.6440 - val_loss: 1.4404 - val_acc: 0.4560\n",
      "Epoch 25/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8958 - acc: 0.6312 - val_loss: 1.4098 - val_acc: 0.4780\n",
      "Epoch 26/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8809 - acc: 0.6495 - val_loss: 1.4507 - val_acc: 0.4478\n",
      "Epoch 27/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8903 - acc: 0.6294 - val_loss: 1.4084 - val_acc: 0.4643\n",
      "Epoch 28/300\n",
      "545/545 [==============================] - 0s 86us/step - loss: 0.8906 - acc: 0.6239 - val_loss: 1.4441 - val_acc: 0.4396\n",
      "Epoch 29/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.9186 - acc: 0.6239 - val_loss: 1.4153 - val_acc: 0.4560\n",
      "Epoch 30/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8780 - acc: 0.6367 - val_loss: 1.4333 - val_acc: 0.4451\n",
      "Epoch 31/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.9107 - acc: 0.6092 - val_loss: 1.3599 - val_acc: 0.4670\n",
      "Epoch 32/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8822 - acc: 0.6367 - val_loss: 1.3260 - val_acc: 0.4890\n",
      "Epoch 33/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8910 - acc: 0.6514 - val_loss: 1.2902 - val_acc: 0.5000\n",
      "Epoch 34/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8748 - acc: 0.6440 - val_loss: 1.4762 - val_acc: 0.4890\n",
      "Epoch 35/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.9104 - acc: 0.6404 - val_loss: 1.4117 - val_acc: 0.4780\n",
      "Epoch 36/300\n",
      "545/545 [==============================] - 0s 88us/step - loss: 0.8817 - acc: 0.6404 - val_loss: 1.3191 - val_acc: 0.4780\n",
      "Epoch 37/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8843 - acc: 0.6312 - val_loss: 1.3566 - val_acc: 0.4808\n",
      "Epoch 38/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.9013 - acc: 0.6220 - val_loss: 1.3235 - val_acc: 0.4945\n",
      "Epoch 39/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8895 - acc: 0.6330 - val_loss: 1.3747 - val_acc: 0.4753\n",
      "Epoch 40/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8872 - acc: 0.6404 - val_loss: 1.3323 - val_acc: 0.4808\n",
      "Epoch 41/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8860 - acc: 0.6312 - val_loss: 1.4091 - val_acc: 0.4753\n",
      "Epoch 42/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8977 - acc: 0.6312 - val_loss: 1.3527 - val_acc: 0.4863\n",
      "Epoch 43/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8896 - acc: 0.6349 - val_loss: 1.4181 - val_acc: 0.4725\n",
      "Epoch 44/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8749 - acc: 0.6459 - val_loss: 1.3524 - val_acc: 0.4615\n",
      "Epoch 45/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8939 - acc: 0.6257 - val_loss: 1.3829 - val_acc: 0.4945\n",
      "Epoch 46/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8969 - acc: 0.6239 - val_loss: 1.3393 - val_acc: 0.4780\n",
      "Epoch 47/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.9114 - acc: 0.6128 - val_loss: 1.3185 - val_acc: 0.5027\n",
      "Epoch 48/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8775 - acc: 0.6367 - val_loss: 1.3788 - val_acc: 0.4863\n",
      "Epoch 49/300\n",
      "545/545 [==============================] - 0s 88us/step - loss: 0.9006 - acc: 0.6257 - val_loss: 1.4741 - val_acc: 0.4368\n",
      "Epoch 50/300\n",
      "545/545 [==============================] - 0s 88us/step - loss: 0.8929 - acc: 0.6294 - val_loss: 1.4691 - val_acc: 0.4725\n",
      "Epoch 51/300\n",
      "545/545 [==============================] - 0s 90us/step - loss: 0.9021 - acc: 0.6202 - val_loss: 1.3574 - val_acc: 0.5000\n",
      "Epoch 52/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8940 - acc: 0.6128 - val_loss: 1.3753 - val_acc: 0.4890\n",
      "Epoch 53/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8741 - acc: 0.6459 - val_loss: 1.3930 - val_acc: 0.4643\n",
      "Epoch 54/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8723 - acc: 0.6477 - val_loss: 1.4524 - val_acc: 0.4863\n",
      "Epoch 55/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.9004 - acc: 0.6183 - val_loss: 1.3473 - val_acc: 0.4890\n",
      "Epoch 56/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8737 - acc: 0.6312 - val_loss: 1.3129 - val_acc: 0.5055\n",
      "Epoch 57/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8899 - acc: 0.6514 - val_loss: 1.3115 - val_acc: 0.4890\n",
      "Epoch 58/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8717 - acc: 0.6495 - val_loss: 1.3579 - val_acc: 0.4753\n",
      "Epoch 59/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8754 - acc: 0.6514 - val_loss: 1.3438 - val_acc: 0.4698\n",
      "Epoch 60/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8707 - acc: 0.6294 - val_loss: 1.4015 - val_acc: 0.4505\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 0s 79us/step - loss: 0.8971 - acc: 0.6147 - val_loss: 1.4480 - val_acc: 0.4423\n",
      "Epoch 62/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.9000 - acc: 0.6220 - val_loss: 1.3796 - val_acc: 0.4780\n",
      "Epoch 63/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8734 - acc: 0.6294 - val_loss: 1.4109 - val_acc: 0.4753\n",
      "Epoch 64/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8605 - acc: 0.6385 - val_loss: 1.4326 - val_acc: 0.4780\n",
      "Epoch 65/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8768 - acc: 0.6312 - val_loss: 1.4295 - val_acc: 0.4588\n",
      "Epoch 66/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8636 - acc: 0.6330 - val_loss: 1.3416 - val_acc: 0.4890\n",
      "Epoch 67/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8796 - acc: 0.6385 - val_loss: 1.3244 - val_acc: 0.4863\n",
      "Epoch 68/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8727 - acc: 0.6422 - val_loss: 1.4200 - val_acc: 0.4780\n",
      "Epoch 69/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8586 - acc: 0.6532 - val_loss: 1.4160 - val_acc: 0.4808\n",
      "Epoch 70/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8768 - acc: 0.6532 - val_loss: 1.3809 - val_acc: 0.4918\n",
      "Epoch 71/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8597 - acc: 0.6679 - val_loss: 1.3696 - val_acc: 0.4890\n",
      "Epoch 72/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8858 - acc: 0.6459 - val_loss: 1.4332 - val_acc: 0.4615\n",
      "Epoch 73/300\n",
      "545/545 [==============================] - 0s 88us/step - loss: 0.8728 - acc: 0.6514 - val_loss: 1.4456 - val_acc: 0.4725\n",
      "Epoch 74/300\n",
      "545/545 [==============================] - 0s 88us/step - loss: 0.8766 - acc: 0.6349 - val_loss: 1.4249 - val_acc: 0.4753\n",
      "Epoch 75/300\n",
      "545/545 [==============================] - 0s 84us/step - loss: 0.8897 - acc: 0.6183 - val_loss: 1.4035 - val_acc: 0.4368\n",
      "Epoch 76/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8634 - acc: 0.6422 - val_loss: 1.3087 - val_acc: 0.4973\n",
      "Epoch 77/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8593 - acc: 0.6385 - val_loss: 1.3609 - val_acc: 0.4835\n",
      "Epoch 78/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.9134 - acc: 0.6147 - val_loss: 1.3646 - val_acc: 0.4753\n",
      "Epoch 79/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8697 - acc: 0.6679 - val_loss: 1.3412 - val_acc: 0.4780\n",
      "Epoch 80/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8704 - acc: 0.6624 - val_loss: 1.3944 - val_acc: 0.4890\n",
      "Epoch 81/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8866 - acc: 0.6385 - val_loss: 1.3589 - val_acc: 0.4918\n",
      "Epoch 82/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8668 - acc: 0.6422 - val_loss: 1.3877 - val_acc: 0.4643\n",
      "Epoch 83/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8726 - acc: 0.6532 - val_loss: 1.3445 - val_acc: 0.4863\n",
      "Epoch 84/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8707 - acc: 0.6330 - val_loss: 1.3775 - val_acc: 0.4725\n",
      "Epoch 85/300\n",
      "545/545 [==============================] - 0s 86us/step - loss: 0.8612 - acc: 0.6587 - val_loss: 1.3995 - val_acc: 0.4643\n",
      "Epoch 86/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8882 - acc: 0.6624 - val_loss: 1.4405 - val_acc: 0.4725\n",
      "Epoch 87/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8948 - acc: 0.6349 - val_loss: 1.3819 - val_acc: 0.4753\n",
      "Epoch 88/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8741 - acc: 0.6495 - val_loss: 1.4701 - val_acc: 0.4478\n",
      "Epoch 89/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8701 - acc: 0.6330 - val_loss: 1.4073 - val_acc: 0.4918\n",
      "Epoch 90/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8651 - acc: 0.6477 - val_loss: 1.4171 - val_acc: 0.4560\n",
      "Epoch 91/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8734 - acc: 0.6330 - val_loss: 1.3673 - val_acc: 0.4835\n",
      "Epoch 92/300\n",
      "545/545 [==============================] - 0s 73us/step - loss: 0.8665 - acc: 0.6257 - val_loss: 1.3663 - val_acc: 0.4753\n",
      "Epoch 93/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8848 - acc: 0.6495 - val_loss: 1.3580 - val_acc: 0.4890\n",
      "Epoch 94/300\n",
      "545/545 [==============================] - 0s 73us/step - loss: 0.8956 - acc: 0.6239 - val_loss: 1.4571 - val_acc: 0.4615\n",
      "Epoch 95/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8693 - acc: 0.6422 - val_loss: 1.3651 - val_acc: 0.4780\n",
      "Epoch 96/300\n",
      "545/545 [==============================] - 0s 81us/step - loss: 0.8709 - acc: 0.6569 - val_loss: 1.4061 - val_acc: 0.4808\n",
      "Epoch 97/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8776 - acc: 0.6422 - val_loss: 1.5020 - val_acc: 0.4588\n",
      "Epoch 98/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8622 - acc: 0.6550 - val_loss: 1.4014 - val_acc: 0.4835\n",
      "Epoch 99/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8929 - acc: 0.6330 - val_loss: 1.3845 - val_acc: 0.4396\n",
      "Epoch 100/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8608 - acc: 0.6495 - val_loss: 1.3593 - val_acc: 0.5330\n",
      "Epoch 101/300\n",
      "545/545 [==============================] - 0s 73us/step - loss: 0.8703 - acc: 0.6477 - val_loss: 1.4240 - val_acc: 0.4670\n",
      "Epoch 102/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8562 - acc: 0.6550 - val_loss: 1.3906 - val_acc: 0.4753\n",
      "Epoch 103/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8724 - acc: 0.6587 - val_loss: 1.4146 - val_acc: 0.4890\n",
      "Epoch 104/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8886 - acc: 0.6239 - val_loss: 1.3361 - val_acc: 0.4973\n",
      "Epoch 105/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8947 - acc: 0.6294 - val_loss: 1.4224 - val_acc: 0.4725\n",
      "Epoch 106/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8659 - acc: 0.6642 - val_loss: 1.3949 - val_acc: 0.4835\n",
      "Epoch 107/300\n",
      "545/545 [==============================] - 0s 83us/step - loss: 0.8744 - acc: 0.6514 - val_loss: 1.3597 - val_acc: 0.4890\n",
      "Epoch 108/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8679 - acc: 0.6514 - val_loss: 1.4406 - val_acc: 0.4286\n",
      "Epoch 109/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8918 - acc: 0.6440 - val_loss: 1.3473 - val_acc: 0.5000\n",
      "Epoch 110/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8908 - acc: 0.6569 - val_loss: 1.4170 - val_acc: 0.4588\n",
      "Epoch 111/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8808 - acc: 0.6330 - val_loss: 1.3666 - val_acc: 0.4835\n",
      "Epoch 112/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8695 - acc: 0.6624 - val_loss: 1.4181 - val_acc: 0.4725\n",
      "Epoch 113/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8781 - acc: 0.6569 - val_loss: 1.3531 - val_acc: 0.4780\n",
      "Epoch 114/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8934 - acc: 0.6312 - val_loss: 1.3802 - val_acc: 0.5000\n",
      "Epoch 115/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8730 - acc: 0.6514 - val_loss: 1.3722 - val_acc: 0.4698\n",
      "Epoch 116/300\n",
      "545/545 [==============================] - 0s 73us/step - loss: 0.8578 - acc: 0.6495 - val_loss: 1.3687 - val_acc: 0.4945\n",
      "Epoch 117/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8687 - acc: 0.6459 - val_loss: 1.3576 - val_acc: 0.4835\n",
      "Epoch 118/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8851 - acc: 0.6385 - val_loss: 1.3979 - val_acc: 0.4505\n",
      "Epoch 119/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8755 - acc: 0.6092 - val_loss: 1.5634 - val_acc: 0.4725\n",
      "Epoch 120/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8780 - acc: 0.6257 - val_loss: 1.5361 - val_acc: 0.4451\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 0s 77us/step - loss: 0.8605 - acc: 0.6440 - val_loss: 1.3684 - val_acc: 0.5165\n",
      "Epoch 122/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8842 - acc: 0.6404 - val_loss: 1.3504 - val_acc: 0.4945\n",
      "Epoch 123/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8767 - acc: 0.6220 - val_loss: 1.3609 - val_acc: 0.4945\n",
      "Epoch 124/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8690 - acc: 0.6404 - val_loss: 1.3350 - val_acc: 0.5137\n",
      "Epoch 125/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8542 - acc: 0.6367 - val_loss: 1.3736 - val_acc: 0.4615\n",
      "Epoch 126/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8624 - acc: 0.6514 - val_loss: 1.3852 - val_acc: 0.4780\n",
      "Epoch 127/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8545 - acc: 0.6569 - val_loss: 1.4375 - val_acc: 0.4423\n",
      "Epoch 128/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8822 - acc: 0.6404 - val_loss: 1.3600 - val_acc: 0.4890\n",
      "Epoch 129/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8640 - acc: 0.6514 - val_loss: 1.3642 - val_acc: 0.4780\n",
      "Epoch 130/300\n",
      "545/545 [==============================] - 0s 79us/step - loss: 0.8751 - acc: 0.6404 - val_loss: 1.3995 - val_acc: 0.4780\n",
      "Epoch 131/300\n",
      "545/545 [==============================] - 0s 75us/step - loss: 0.8843 - acc: 0.6385 - val_loss: 1.3991 - val_acc: 0.4808\n",
      "Epoch 132/300\n",
      "545/545 [==============================] - 0s 77us/step - loss: 0.8651 - acc: 0.6349 - val_loss: 1.3547 - val_acc: 0.5027\n",
      "Epoch 133/300\n",
      "545/545 [==============================] - 0s 73us/step - loss: 0.8610 - acc: 0.6459 - val_loss: 1.3239 - val_acc: 0.5165\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_split = 0.4, epochs=300, batch_size=10, \n",
    "                    callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import math\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]  prediction [2.46036885e-10 2.66174444e-10 9.72912972e-10 1.84731420e-35\n",
      " 9.12664272e-03 3.18776995e-01 5.60273468e-01 1.01238914e-01\n",
      " 1.05838859e-02 3.93954591e-11]\n",
      "실제 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  prediction [1.6409579e-18 3.2355490e-15 5.3800685e-16 0.0000000e+00 4.7296151e-03\n",
      " 4.8150942e-01 4.8301002e-01 2.0752270e-02 9.9986456e-03 4.2347142e-19]\n",
      "실제 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  prediction [2.6228333e-14 1.7156833e-13 5.2287746e-15 0.0000000e+00 1.2818093e-03\n",
      " 3.9821991e-01 5.8733523e-01 1.2449009e-02 7.1410619e-04 9.9049471e-14]\n",
      "실제 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  prediction [9.4608675e-18 9.5928250e-14 6.6269153e-16 0.0000000e+00 2.0640967e-03\n",
      " 1.3032749e-01 3.9536545e-01 4.4326511e-01 2.8977793e-02 9.8304354e-17]\n",
      "실제 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]  prediction [2.4788106e-11 1.3273224e-12 2.0727756e-11 2.9487340e-19 6.4227693e-03\n",
      " 5.9002137e-01 3.8058311e-01 2.2082238e-02 8.9050893e-04 1.4665717e-12]\n",
      "실제 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]  prediction [3.7817854e-10 8.6888954e-11 5.3610355e-10 4.5666726e-26 1.7306940e-05\n",
      " 5.2568987e-03 7.7247721e-01 1.3973886e-01 8.2509696e-02 2.8950462e-11]\n",
      "실제 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  prediction [8.7558541e-13 1.7461029e-11 7.0371102e-13 0.0000000e+00 1.6227196e-03\n",
      " 4.3340638e-01 4.4221458e-01 1.0975002e-01 1.3006367e-02 2.8437159e-13]\n",
      "실제 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]  prediction [3.5005340e-13 1.3027929e-13 5.1114261e-13 6.8668039e-30 2.9197426e-07\n",
      " 3.1449640e-04 9.3770355e-01 3.9442334e-02 2.2539336e-02 1.2271167e-13]\n",
      "실제 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  prediction [4.3703707e-15 5.4087805e-13 3.1864609e-15 0.0000000e+00 1.8037310e-01\n",
      " 6.7381728e-01 1.3590619e-01 9.8716747e-03 3.1748747e-05 1.5961291e-14]\n",
      "실제 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]  prediction [6.8029124e-12 1.5882899e-12 4.0551364e-12 1.6768821e-14 2.8607514e-05\n",
      " 8.1185056e-03 9.4084436e-01 5.0996330e-02 1.2238216e-05 4.6763001e-12]\n"
     ]
    }
   ],
   "source": [
    "Y_prediction = model.predict(X_test)\n",
    "\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print (\"실제\",label, \" prediction\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['loss']\n",
    "y_acc = history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
